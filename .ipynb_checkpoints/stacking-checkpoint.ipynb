{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/obaidur/software/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn import svm\n",
    "import lightgbm as lgb\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import skew\n",
    "from scipy.stats.stats import pearsonr\n",
    "from scipy import stats\n",
    "import xgboost as xgb\n",
    "from scipy.stats import norm\n",
    "#from pyglmnet import GLM # Marco: need to understand how to install this \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from subprocess import call\n",
    "from sklearn.cross_validation import KFold\n",
    "#from sklearn.model_selection import KFold\n",
    "\n",
    "from IPython import get_ipython\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Ensemble class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Ensemble(object):\n",
    "    def __init__(self, n_folds, base_models,seed_value):\n",
    "        self.n_folds = n_folds\n",
    "        self.base_models = base_models\n",
    "        self.seed_value = seed_value\n",
    "        \n",
    "    def fit_predict(self, X, y, T):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        T = np.array(T)\n",
    "        \n",
    "        # Let's define the folds based on the length of y and number of folds requested\n",
    "        \n",
    "        folds = list(KFold(len(y), n_folds=self.n_folds, shuffle=True, random_state=self.seed_value))\n",
    "        #folds = KFold(n_splits=self.n_folds, shuffle=True, random_state=self.seed_value)\n",
    "        \n",
    "        print(\"folds=\",folds)\n",
    "        \n",
    "        S_train = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        S_test = np.zeros((T.shape[0], len(self.base_models)))\n",
    "        \n",
    "        for i, reg in enumerate(self.base_models):\n",
    "            S_test_i = np.zeros((T.shape[0], len(folds)))\n",
    "            for j, (train_idx, test_idx) in enumerate(folds):\n",
    "                X_train = X[train_idx]\n",
    "                y_train = y[train_idx]\n",
    "                \n",
    "                # a part of the training set is held out\n",
    "                X_holdout = X[test_idx]\n",
    "                # y_holdout = y[test_idx]\n",
    "                reg.fit(X_train, y_train)\n",
    "                y_pred = reg.predict(X_holdout)[:]\n",
    "                # Now the predicted values on the heldout set is used as the training set for the stacker model\n",
    "                S_train[test_idx, i] = y_pred\n",
    "                S_test_i[:, j] = reg.predict(T)[:]\n",
    "            S_test[:, i] = S_test_i.mean(1)\n",
    "        \n",
    "        '''\n",
    "        # Cross validate the stacker model\n",
    "        stackermodel = xgb.XGBRegressor()\n",
    "\n",
    "        # dict with tunning parameters\n",
    "        param_grid = {\n",
    "        'max_depth': [2, 4], \n",
    "        'learning_rate': [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3],\n",
    "        'min_child_weight': range(1, 10, 2),\n",
    "        'n_estimators': range(50, 300, 50),\n",
    "        'objective': ['reg:linear']\n",
    "        }\n",
    "\n",
    "        #kfold = KFold(n_splits=nfold, random_state=seed)\n",
    "\n",
    "        scorer = make_scorer(rmse, greater_is_better=False)\n",
    "        grid_search = GridSearchCV(stackermodel, param_grid, n_jobs=-1, cv=5, verbose=1, scoring=scorer)\n",
    "        grid_result = grid_search.fit(S_train, y)\n",
    "\n",
    "        means = grid_result.cv_results_['mean_test_score']\n",
    "        stds = grid_result.cv_results_['std_test_score']\n",
    "        params = grid_result.cv_results_['params']\n",
    "\n",
    "        for mean, stdev, param in zip(means, stds, params):\n",
    "        #print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "            print(\"{:06.5f} ({:06.5f}) with {}\".format(mean, stdev, param))\n",
    "\n",
    "        # summarize results\n",
    "        print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "        '''\n",
    "        # Now fit the stacker model\n",
    "        stackermodel = xgb.XGBRegressor(n_estimators=100,learning_rate=0.1,max_depth=2,min_child_weight=5,objective='reg:linear')\n",
    "        stackermodel.fit(S_train, y)\n",
    "        y_pred = stackermodel.predict(S_test)[:]\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./models/single/model_elasticnet.py\n",
      "./models/single/model_et.py\n",
      "./models/single/model_lgb.py\n",
      "./models/single/model_rf.py\n",
      "./models/single/model_xgb.py\n",
      "['./models/single/model_elasticnet.py', './models/single/model_et.py', './models/single/model_lgb.py', './models/single/model_rf.py', './models/single/model_xgb.py']\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "seed = 2017\n",
    "nfold = 5\n",
    "bmodels = [\"elasticnet\",\"et\",\"lgb\",\"rf\",\"xgb\"]\n",
    "    \n",
    "train = pd.read_csv(\"./data/X_train_v2.csv\")\n",
    "y = train['SalePrice']\n",
    "X = train.loc[:,'MSSubClass':'SaleCondition_Partial']\n",
    "    \n",
    "test = pd.read_csv(\"./data/X_test_v2.csv\")\n",
    "id = test[\"Id\"]\n",
    "T = test.loc[:,'MSSubClass':'SaleCondition_Partial']\n",
    "    \n",
    "#Set the base models\n",
    "\n",
    "base_models_name = []\n",
    "for j in range(len(bmodels)):\n",
    "        modelname = (\"./models/single/model_\" + bmodels[j] + \".py\")\n",
    "        print(modelname)\n",
    "        base_models_name.append(modelname)\n",
    "\n",
    "print(base_models_name)\n",
    "base_models = []\n",
    "    \n",
    "for i, bm in enumerate(base_models_name):\n",
    "        model = !grep \"model =\" {bm}\n",
    "        model = model[0]\n",
    "        model = model[12:]\n",
    "        model = eval(model)\n",
    "        base_models.append(model)\n",
    "#print(model)\n",
    "#print(base_models) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#xgb = xgb.XGBRegressor(n_estimators=250,learning_rate=0.1,max_depth=4,min_child_weight=1,objective='reg:linear')\n",
    "# Call stacking\n",
    "    \n",
    "ens = Ensemble(n_folds=nfold, base_models=base_models,seed_value=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folds= [(array([   0,    1,    2, ..., 1436, 1437, 1438]), array([  17,   20,   25,   41,   54,   55,   56,   57,   61,   64,   93,\n",
      "        100,  102,  114,  115,  116,  117,  119,  120,  123,  128,  160,\n",
      "        166,  168,  179,  183,  188,  189,  191,  192,  196,  213,  216,\n",
      "        222,  230,  232,  235,  238,  239,  243,  257,  261,  262,  263,\n",
      "        270,  271,  273,  277,  282,  296,  299,  300,  313,  317,  319,\n",
      "        331,  332,  333,  350,  353,  356,  358,  364,  371,  374,  375,\n",
      "        377,  379,  383,  389,  391,  397,  401,  411,  413,  414,  416,\n",
      "        421,  423,  426,  433,  434,  435,  436,  441,  443,  449,  451,\n",
      "        457,  460,  461,  462,  463,  464,  465,  478,  480,  481,  484,\n",
      "        487,  492,  497,  499,  505,  510,  522,  530,  534,  536,  539,\n",
      "        544,  546,  554,  555,  556,  557,  570,  571,  575,  577,  601,\n",
      "        609,  617,  623,  636,  639,  642,  649,  650,  651,  652,  657,\n",
      "        668,  679,  681,  684,  692,  698,  701,  714,  717,  718,  723,\n",
      "        727,  735,  737,  738,  749,  751,  759,  764,  779,  780,  792,\n",
      "        795,  798,  800,  806,  812,  828,  835,  838,  843,  856,  861,\n",
      "        867,  868,  869,  873,  875,  876,  879,  880,  887,  892,  893,\n",
      "        912,  918,  934,  936,  944,  946,  950,  952,  955,  982,  983,\n",
      "        987,  988,  991,  993,  997,  998, 1004, 1009, 1012, 1018, 1024,\n",
      "       1025, 1026, 1028, 1030, 1037, 1043, 1045, 1054, 1055, 1056, 1059,\n",
      "       1065, 1068, 1087, 1089, 1091, 1092, 1104, 1108, 1113, 1115, 1116,\n",
      "       1122, 1134, 1140, 1142, 1152, 1160, 1175, 1177, 1180, 1186, 1190,\n",
      "       1192, 1194, 1199, 1201, 1203, 1213, 1215, 1217, 1218, 1231, 1232,\n",
      "       1237, 1253, 1254, 1259, 1264, 1269, 1272, 1281, 1285, 1292, 1295,\n",
      "       1299, 1300, 1302, 1305, 1306, 1311, 1316, 1320, 1322, 1324, 1325,\n",
      "       1337, 1342, 1344, 1348, 1349, 1353, 1354, 1357, 1371, 1373, 1382,\n",
      "       1385, 1392, 1398, 1407, 1408, 1410, 1411, 1412, 1415, 1425, 1428,\n",
      "       1430, 1435])), (array([   2,    3,    4, ..., 1436, 1437, 1438]), array([   0,    1,    5,   11,   15,   18,   21,   30,   33,   36,   42,\n",
      "         59,   60,   63,   67,   72,   76,   78,   81,   82,   86,   94,\n",
      "         97,  104,  106,  108,  111,  121,  134,  138,  140,  148,  149,\n",
      "        150,  155,  157,  161,  162,  165,  174,  181,  186,  190,  194,\n",
      "        197,  202,  206,  207,  211,  214,  218,  219,  221,  237,  241,\n",
      "        247,  249,  259,  285,  287,  288,  308,  311,  312,  314,  323,\n",
      "        324,  336,  340,  347,  351,  355,  360,  361,  362,  363,  368,\n",
      "        381,  382,  387,  390,  396,  399,  400,  402,  417,  420,  429,\n",
      "        430,  432,  438,  452,  467,  472,  473,  476,  482,  485,  501,\n",
      "        503,  509,  512,  516,  517,  523,  538,  548,  550,  551,  552,\n",
      "        558,  561,  562,  563,  580,  585,  597,  603,  605,  618,  621,\n",
      "        624,  625,  632,  635,  638,  640,  644,  647,  656,  662,  665,\n",
      "        687,  690,  693,  697,  706,  710,  712,  726,  728,  733,  734,\n",
      "        742,  744,  747,  757,  758,  763,  765,  766,  773,  776,  777,\n",
      "        783,  787,  788,  791,  794,  809,  811,  818,  819,  821,  829,\n",
      "        830,  833,  837,  842,  849,  853,  857,  864,  866,  870,  890,\n",
      "        891,  905,  907,  908,  914,  917,  920,  922,  923,  927,  931,\n",
      "        932,  937,  938,  941,  943,  962,  963,  981,  986,  999, 1000,\n",
      "       1005, 1008, 1014, 1015, 1017, 1020, 1029, 1032, 1033, 1034, 1035,\n",
      "       1038, 1044, 1046, 1060, 1063, 1069, 1075, 1085, 1086, 1090, 1096,\n",
      "       1099, 1100, 1101, 1103, 1105, 1120, 1125, 1138, 1144, 1146, 1150,\n",
      "       1165, 1166, 1172, 1174, 1176, 1178, 1179, 1181, 1182, 1184, 1187,\n",
      "       1189, 1204, 1205, 1207, 1212, 1219, 1223, 1227, 1230, 1248, 1256,\n",
      "       1258, 1262, 1265, 1268, 1271, 1280, 1282, 1286, 1289, 1291, 1293,\n",
      "       1296, 1321, 1330, 1335, 1340, 1341, 1345, 1355, 1358, 1363, 1365,\n",
      "       1370, 1372, 1379, 1384, 1386, 1387, 1388, 1391, 1394, 1396, 1404,\n",
      "       1421, 1424])), (array([   0,    1,    2, ..., 1434, 1435, 1437]), array([   7,   10,   12,   19,   24,   27,   32,   34,   40,   43,   44,\n",
      "         45,   48,   50,   52,   53,   62,   71,   84,   89,   92,   95,\n",
      "         96,   98,  101,  107,  118,  129,  132,  133,  136,  139,  145,\n",
      "        146,  147,  151,  154,  159,  163,  164,  170,  173,  177,  182,\n",
      "        184,  185,  187,  201,  210,  217,  226,  227,  229,  233,  236,\n",
      "        242,  245,  246,  250,  253,  254,  266,  267,  278,  281,  283,\n",
      "        289,  290,  293,  304,  305,  306,  310,  322,  326,  327,  329,\n",
      "        335,  339,  367,  369,  370,  376,  403,  405,  407,  412,  422,\n",
      "        428,  437,  440,  442,  446,  455,  458,  468,  486,  488,  493,\n",
      "        496,  498,  500,  502,  504,  507,  515,  520,  526,  527,  528,\n",
      "        532,  533,  535,  540,  545,  553,  564,  567,  569,  572,  573,\n",
      "        576,  578,  581,  583,  584,  586,  588,  595,  598,  602,  604,\n",
      "        611,  615,  622,  630,  637,  641,  648,  655,  660,  663,  667,\n",
      "        669,  672,  674,  677,  694,  702,  708,  709,  715,  720,  721,\n",
      "        732,  736,  741,  743,  745,  746,  748,  750,  755,  756,  761,\n",
      "        762,  767,  768,  769,  772,  774,  775,  789,  796,  797,  803,\n",
      "        810,  815,  816,  820,  824,  826,  834,  839,  845,  846,  847,\n",
      "        851,  852,  859,  881,  882,  884,  897,  904,  913,  925,  942,\n",
      "        961,  969,  980,  984,  989,  994,  995, 1003, 1019, 1027, 1031,\n",
      "       1042, 1048, 1049, 1057, 1061, 1066, 1067, 1071, 1072, 1074, 1077,\n",
      "       1082, 1083, 1102, 1109, 1117, 1119, 1121, 1123, 1124, 1135, 1136,\n",
      "       1139, 1143, 1145, 1153, 1154, 1162, 1164, 1168, 1170, 1188, 1193,\n",
      "       1197, 1202, 1209, 1211, 1220, 1221, 1228, 1233, 1234, 1239, 1242,\n",
      "       1244, 1247, 1250, 1255, 1261, 1263, 1270, 1277, 1279, 1294, 1297,\n",
      "       1298, 1307, 1309, 1315, 1326, 1327, 1329, 1338, 1343, 1347, 1359,\n",
      "       1369, 1375, 1377, 1380, 1389, 1390, 1395, 1401, 1405, 1419, 1433,\n",
      "       1436, 1438])), (array([   0,    1,    4, ..., 1435, 1436, 1438]), array([   2,    3,   23,   26,   31,   37,   38,   39,   69,   73,   74,\n",
      "         77,   79,   83,   87,   91,   99,  105,  113,  124,  125,  127,\n",
      "        131,  137,  141,  142,  144,  152,  153,  156,  158,  171,  175,\n",
      "        176,  205,  209,  215,  225,  228,  231,  234,  244,  248,  251,\n",
      "        258,  260,  265,  268,  275,  279,  286,  291,  292,  297,  298,\n",
      "        301,  303,  315,  316,  318,  328,  330,  337,  338,  341,  343,\n",
      "        346,  348,  354,  357,  359,  365,  366,  372,  373,  380,  386,\n",
      "        388,  393,  406,  409,  410,  415,  418,  425,  431,  439,  445,\n",
      "        448,  453,  454,  469,  474,  479,  489,  508,  513,  518,  521,\n",
      "        531,  537,  542,  543,  560,  565,  566,  574,  589,  590,  593,\n",
      "        600,  610,  612,  619,  626,  628,  633,  634,  645,  654,  658,\n",
      "        659,  664,  666,  670,  673,  678,  680,  682,  683,  685,  689,\n",
      "        691,  695,  699,  700,  705,  711,  713,  716,  719,  724,  730,\n",
      "        731,  740,  752,  754,  770,  782,  785,  793,  799,  801,  805,\n",
      "        807,  813,  814,  823,  825,  827,  836,  840,  841,  848,  850,\n",
      "        855,  862,  863,  878,  883,  885,  888,  889,  898,  899,  901,\n",
      "        902,  903,  910,  921,  924,  929,  933,  945,  951,  954,  956,\n",
      "        960,  964,  966,  970,  973,  974,  975,  977,  985, 1006, 1007,\n",
      "       1010, 1013, 1016, 1021, 1023, 1040, 1047, 1050, 1051, 1053, 1076,\n",
      "       1079, 1080, 1081, 1084, 1088, 1097, 1107, 1110, 1111, 1112, 1114,\n",
      "       1126, 1128, 1137, 1141, 1147, 1149, 1151, 1155, 1156, 1157, 1158,\n",
      "       1159, 1163, 1169, 1173, 1183, 1185, 1198, 1200, 1206, 1208, 1210,\n",
      "       1216, 1226, 1229, 1236, 1238, 1243, 1245, 1249, 1251, 1257, 1260,\n",
      "       1266, 1267, 1283, 1284, 1287, 1288, 1290, 1303, 1310, 1314, 1317,\n",
      "       1323, 1328, 1331, 1332, 1346, 1350, 1352, 1356, 1360, 1364, 1367,\n",
      "       1378, 1381, 1393, 1400, 1409, 1413, 1414, 1416, 1420, 1423, 1432,\n",
      "       1434, 1437])), (array([   0,    1,    2, ..., 1436, 1437, 1438]), array([   4,    6,    8,    9,   13,   14,   16,   22,   28,   29,   35,\n",
      "         46,   47,   49,   51,   58,   65,   66,   68,   70,   75,   80,\n",
      "         85,   88,   90,  103,  109,  110,  112,  122,  126,  130,  135,\n",
      "        143,  167,  169,  172,  178,  180,  193,  195,  198,  199,  200,\n",
      "        203,  204,  208,  212,  220,  223,  224,  240,  252,  255,  256,\n",
      "        264,  269,  272,  274,  276,  280,  284,  294,  295,  302,  307,\n",
      "        309,  320,  321,  325,  334,  342,  344,  345,  349,  352,  378,\n",
      "        384,  385,  392,  394,  395,  398,  404,  408,  419,  424,  427,\n",
      "        444,  447,  450,  456,  459,  466,  470,  471,  475,  477,  483,\n",
      "        490,  491,  494,  495,  506,  511,  514,  519,  524,  525,  529,\n",
      "        541,  547,  549,  559,  568,  579,  582,  587,  591,  592,  594,\n",
      "        596,  599,  606,  607,  608,  613,  614,  616,  620,  627,  629,\n",
      "        631,  643,  646,  653,  661,  671,  675,  676,  686,  688,  696,\n",
      "        703,  704,  707,  722,  725,  729,  739,  753,  760,  771,  778,\n",
      "        781,  784,  786,  790,  802,  804,  808,  817,  822,  831,  832,\n",
      "        844,  854,  858,  860,  865,  871,  872,  874,  877,  886,  894,\n",
      "        895,  896,  900,  906,  909,  911,  915,  916,  919,  926,  928,\n",
      "        930,  935,  939,  940,  947,  948,  949,  953,  957,  958,  959,\n",
      "        965,  967,  968,  971,  972,  976,  978,  979,  990,  992,  996,\n",
      "       1001, 1002, 1011, 1022, 1036, 1039, 1041, 1052, 1058, 1062, 1064,\n",
      "       1070, 1073, 1078, 1093, 1094, 1095, 1098, 1106, 1118, 1127, 1129,\n",
      "       1130, 1131, 1132, 1133, 1148, 1161, 1167, 1171, 1191, 1195, 1196,\n",
      "       1214, 1222, 1224, 1225, 1235, 1240, 1241, 1246, 1252, 1273, 1274,\n",
      "       1275, 1276, 1278, 1301, 1304, 1308, 1312, 1313, 1318, 1319, 1333,\n",
      "       1334, 1336, 1339, 1351, 1361, 1362, 1366, 1368, 1374, 1376, 1383,\n",
      "       1397, 1399, 1402, 1403, 1406, 1417, 1418, 1422, 1426, 1427, 1429,\n",
      "       1431]))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/obaidur/software/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "results = ens.fit_predict(X,y,T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results= [ 11.71511173  11.99144363  12.14565182 ...,  12.01550674  11.69173813\n",
      "  12.33989429]\n"
     ]
    }
   ],
   "source": [
    "print(\"results=\",results)\n",
    "results = np.expm1(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results= [ 122406.6015625  161367.140625   188272.65625   ...,  165297.25\n",
      "  119578.671875   228636.78125  ]\n",
      "     Id      SalePrice\n",
      "0  1461  122406.601562\n",
      "1  1462  161367.140625\n",
      "2  1463  188272.656250\n",
      "3  1464  195541.812500\n",
      "4  1465  185674.156250\n"
     ]
    }
   ],
   "source": [
    "# Now prepare the final results for submission\n",
    "print(\"results=\",results)\n",
    "results = pd.DataFrame(results)\n",
    "results = pd.concat([id,results], axis=1)\n",
    "results.columns =[\"Id\",\"SalePrice\"]\n",
    "print(results.head())\n",
    "results.to_csv(\"./ensembled_results.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
